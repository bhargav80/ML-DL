{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMggp37/Bd368Or61aggf50",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargav80/ML-DL/blob/main/imdb_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "QuqSfQm2zPo6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Load the word index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Reverse the word index to get a mapping from integers to words\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "# Add special tokens for padding, start of sequence, and unknown words\n",
        "reverse_word_index[0] = '<PAD>'\n",
        "reverse_word_index[1] = '<START>'\n",
        "reverse_word_index[2] = '<UNK>'\n",
        "reverse_word_index[3] = '<UNUSED>'\n"
      ],
      "metadata": {
        "id": "NuBVm1lU6Fv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3516e1f-2d40-4b89-e874-966d4eee720d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_review(encoded_review):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n"
      ],
      "metadata": {
        "id": "3qw6Pj5H6FzP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Decode and print a sample review\n",
        "sample_review = x_train[0]\n",
        "print(\"Encoded review:\", sample_review)\n",
        "print(\"Decoded review:\", decode_review(sample_review))\n",
        "print(\"Label:\", y_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtY_b-563rg",
        "outputId": "8f8cf5a6-73f5-43eb-e194-28942b2582ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "Encoded review: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Decoded review: <START> as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room <UNK> it so heart shows to years of every never going <UNK> help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but <UNK> to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other <UNK> in of seen over landed for anyone of <UNK> br show's to whether from than out themselves history he name half some br of <UNK> odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   label 0 : Negative\n",
        "*   label 1 : Positive\n",
        "\n"
      ],
      "metadata": {
        "id": "t2_tUCNR7VDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create embedding"
      ],
      "metadata": {
        "id": "nsTLhBiH7vZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "from tensorflow.keras import layers\n",
        "embedding = layers.Embedding(input_dim = 10000,\n",
        "                                   embeddings_initializer = \"uniform\",\n",
        "                                   output_dim = 32, input_length = maxlen)"
      ],
      "metadata": {
        "id": "YAQWAnDI63Em"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sentence from training set\n",
        "import random\n",
        "random_sentence = random.choice(x_train)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nEmbedded version:\")\n",
        "random_sentence = tf.expand_dims(random_sentence, axis=0)\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(random_sentence)\n",
        "sample_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-2YyW6U8Y5k",
        "outputId": "8257ed42-09db-494e-be03-7fb9e7f5aab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "[1, 13, 122, 24, 124, 18, 49, 58, 11, 61, 1940, 32, 15, 100, 11, 831, 30, 573, 44, 14, 22, 190, 4, 771, 7, 231, 6, 22, 16, 24, 51, 11, 192, 2266, 61, 692, 51, 93, 14, 1270, 431, 31, 4, 91, 423, 108, 60, 8, 14, 55, 251, 15, 13, 28, 126, 110, 16, 7, 4, 2, 9856, 5, 4, 2898, 8, 28, 3009, 11, 41, 2, 2469, 14, 22, 9, 24, 210, 51, 12, 186, 5, 15, 9, 382, 17, 12, 144, 30, 190, 13, 2488, 135, 195, 18, 4, 3158, 2, 34, 6334, 6726, 11, 4824, 7, 4, 6336, 15, 29, 99, 62, 7, 9498, 27, 113, 8, 607, 39, 7623, 7, 4, 432, 15, 36, 5, 4, 2, 71, 5907, 19, 4, 2, 7, 14, 2, 604, 10, 10, 8, 30, 813, 6334, 6726, 9, 4, 243, 7, 2267, 25, 238, 1467, 142, 44, 5, 95, 25, 106, 14, 2, 2607, 24, 64, 11, 257, 85, 21, 11, 41, 2, 4, 1652, 7, 1831, 13, 104, 18, 32, 7, 27, 4021, 18, 278, 5, 4, 141, 15, 15, 109, 16, 87, 6, 8654, 2510, 8224, 1631, 37, 122, 24, 124, 1092, 13, 104, 2107, 16, 321, 11, 14, 217, 371, 6, 55, 2250, 7700, 93, 1444, 34, 4, 6336, 15, 29, 4362, 13, 115, 421, 2, 16, 6, 6874, 11, 11, 192, 50, 16, 38, 76, 4454, 11, 50, 2, 15, 25, 238, 30, 73, 24, 8, 168, 8, 491, 88, 12, 9, 131, 64, 6, 431, 51, 81, 13, 384, 14, 431, 9, 131, 64, 6, 1270, 431, 5, 40, 4, 211, 11, 63, 134, 687, 193, 273, 17, 73, 17, 54, 4, 431, 16, 165, 93, 1754, 6, 168, 33, 89, 183, 71, 224, 95, 5, 51, 42, 138, 50, 26, 38, 111, 275, 4704, 17, 8, 14, 1270, 431, 80, 6681, 129, 692, 199, 2, 5, 2, 26, 3215, 11, 50, 8774, 19, 199, 349, 39, 58, 8, 58, 3760, 6, 2, 3922, 17, 8, 50, 2632, 17, 349, 11, 4, 2469, 7, 41, 2, 134, 289, 306, 8, 721, 183, 125, 247, 73, 5, 13, 264, 12, 9, 6, 4507, 60, 737, 2, 54, 2, 47, 6, 1304, 19, 4205, 42, 38, 12, 62, 306, 64, 8, 28, 1949, 17, 25, 62, 28, 12, 2, 9, 12, 867, 13, 92, 124, 13, 104, 12, 9, 55, 3345, 54, 4, 1169, 260, 3954, 4, 2, 7, 341, 11, 2, 5, 36, 26, 8056, 50, 2, 17, 36, 26, 6428, 4, 293, 6280, 8, 4, 2, 1208, 54, 2, 2080, 4, 85, 107, 8, 124, 15, 29, 9, 1200, 4, 2469, 5, 397, 1021, 5, 170, 83, 4, 3384, 970, 2, 560, 29, 100, 1913, 56, 18, 160, 790, 153, 12, 80, 97, 6, 132, 46, 7, 90, 13, 40, 15, 5205, 10, 10, 13, 92, 104, 50, 9, 101, 824, 17, 8, 43, 51, 12, 817, 8, 28, 2510, 2, 8224, 1340, 8544, 129, 55, 118, 4993, 121, 127, 14, 130, 11, 192, 12, 203, 115, 130, 148, 4993, 26, 38, 73, 2611, 17, 8, 51, 9, 674, 11, 14, 182, 15, 13, 358, 14, 431, 639, 17, 76, 17, 382, 13, 510, 4, 431, 54, 13, 16, 747, 153, 154, 13, 69, 115, 573, 44, 4, 2, 7, 14, 22, 56, 8, 1033, 54, 13, 435, 83, 479, 5, 258, 4, 1618, 44, 2, 50, 9, 179, 6, 52, 855, 8, 850, 190, 280, 32, 9, 301, 5, 224, 44, 4, 1379, 5091, 7, 4, 2, 7, 2, 14, 1270, 431, 304, 23, 6, 489, 15, 13, 5504, 8, 17, 1663, 14, 9, 6, 55, 1202, 589, 8, 2833, 6, 2607, 11, 51, 9, 112, 3367, 13, 104, 14, 9, 35, 321, 22, 742, 4531, 526, 10, 10, 50, 9, 6, 171, 5278, 8, 30, 1887, 7, 13, 92, 104, 32, 4, 1618, 80, 2, 19, 479, 190, 54, 4, 3944, 9, 7470, 17, 443, 2, 183, 70, 79, 55, 921, 88, 32, 4, 360, 26, 105, 21, 14, 9, 2, 2, 742, 4531, 435, 120, 4, 350, 8, 2833, 6, 58, 5, 6, 58, 159, 54, 134, 687, 165, 3860, 4, 1618, 9, 1202, 1444, 5, 12, 80, 24, 64, 2514, 25, 11, 21, 25, 80, 359, 8, 391, 44, 138, 75, 38, 119, 5959, 5143, 50, 9, 11, 4, 3744, 4, 328, 106, 9, 46, 11, 1011, 5, 36, 26, 6428, 6, 91, 813, 7623, 5, 614, 4108, 894, 4, 2, 70, 30, 2815, 6334, 6726, 9, 615, 4954, 5, 2, 17, 73, 17, 2, 26, 6075, 5143, 260, 6, 933, 4529, 33, 4, 2810, 7, 27, 145, 17, 4, 959, 7, 6, 2, 7681, 1685, 83, 27, 648, 39, 496, 9, 56, 8, 4, 4659, 7, 260, 8, 3081, 4, 4408, 7, 8153, 7623, 19, 6, 781, 1517, 7, 263, 91, 2, 5219, 11, 14, 113, 58, 5143, 1364, 9099, 5, 1030, 8, 2407, 4, 2, 63, 9524, 17, 4, 350, 7, 4, 2, 3723, 4, 481, 7, 5959, 2, 8741, 2080, 4, 6428, 1272, 8, 30, 2, 6, 55, 1058, 2407, 985, 4741, 5, 4, 2543, 9, 2, 12, 9, 38, 905, 5, 3094, 1061, 2720, 15, 17, 5959, 5143, 4585, 351, 23, 6, 2446, 7, 3339, 63, 27, 3985, 2, 648, 150, 287, 6334, 6726, 560, 52, 157, 1631, 13, 92, 124, 7, 101, 53, 905, 561, 885, 31, 121, 75, 850, 51, 3552, 817, 95, 54, 4, 2, 9, 2, 7, 4, 8153, 7623, 10, 10, 4, 130, 9, 230, 39, 112, 1210, 7754, 12, 9, 4, 979, 7, 37, 5959, 5143, 9, 5, 51, 29, 817, 150, 8, 4, 2, 349, 11, 6336, 18, 937, 29, 1688, 9498, 2, 694, 27, 483, 5, 1643, 4, 4408, 8, 193, 459, 7, 27, 2, 6806, 5, 14, 166, 2, 179, 3521, 19, 4, 4408, 112, 1202, 1370, 4, 2, 6806, 11, 27, 6599, 8, 30, 3343, 19, 33, 382, 33, 6, 53, 2316, 58, 4, 4408, 560, 33, 4, 273, 121, 150, 32, 26, 8676, 15, 75, 28, 32, 224, 195, 2, 18, 31, 196, 251, 5, 1037, 795, 23, 89, 3521, 50, 2048, 71, 11, 4824, 2, 560, 29, 62, 247, 133, 15, 39, 4, 4408, 74, 79, 6, 2, 2, 14, 9, 6, 55, 8088, 213, 5, 95, 29, 266, 8, 5143, 150, 133, 9, 6, 132, 37, 47, 57, 165, 2668, 38, 13, 244, 170, 8, 2, 90, 6, 2, 5, 27, 403, 3433, 30, 398, 23, 4, 7386, 7, 263, 2, 351, 4, 4894, 9, 332, 17, 151, 12, 16, 43, 7716, 34, 2, 309, 37, 1407, 34, 4, 2, 19, 4, 4408, 5, 4, 360, 7, 4, 349, 5959, 5143, 6220]      \n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1111, 32), dtype=float32, numpy=\n",
              "array([[[-0.02225154, -0.01910671, -0.00807174, ...,  0.01181003,\n",
              "          0.00914836, -0.0241724 ],\n",
              "        [-0.01185092,  0.02366196,  0.02562003, ..., -0.00928037,\n",
              "         -0.00092162,  0.03270018],\n",
              "        [ 0.00424673, -0.0022311 ,  0.02215285, ..., -0.0414647 ,\n",
              "         -0.01463621,  0.00365248],\n",
              "        ...,\n",
              "        [-0.01165631,  0.04402244, -0.03875955, ..., -0.0413723 ,\n",
              "          0.03055468,  0.01112676],\n",
              "        [-0.02204846,  0.02307537,  0.02998001, ..., -0.03872301,\n",
              "         -0.03032658,  0.01996224],\n",
              "        [ 0.04009536,  0.02960226, -0.01340489, ...,  0.03032771,\n",
              "          0.02864926, -0.03770702]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 : Naive Bayes'"
      ],
      "metadata": {
        "id": "ooeQ3Kxtzdid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the training and test datasets\n",
        "x_train_text = [decode_review(review) for review in x_train]\n",
        "x_test_text = [decode_review(review) for review in x_test]"
      ],
      "metadata": {
        "id": "0NSwcATo0sAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Create tokenization and modelling pipeline\n",
        "model_1 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_1.fit(x_train_text,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "LsaF86XyzWK6",
        "outputId": "74879407-b0ab-489b-e91b-4cc8884490e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "baseline_preds = model_1.predict(x_test_text)\n",
        "baseline_preds[:20]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ8XTeCFzWOI",
        "outputId": "67d0a16e-c0a4-47cb-963e-d421518e6dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a sample review (let's take one from the test set for example)\n",
        "sample_review = x_test[1]\n",
        "sample_review_text = decode_review(sample_review)\n",
        "\n",
        "print(f\"Sample Review Text: \\n{sample_review_text}\\n\",\"class: \",{y_test[1]})\n",
        "\n",
        "# Predict the sentiment of the sample review\n",
        "sample_prediction = model_0.predict([sample_review_text])\n",
        "print(f\"Predicted Sentiment: {'Positive' if sample_prediction[0] == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0_3BqgTzWQt",
        "outputId": "2e6e44f4-b93f-476d-87a9-a9770ace8b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Review Text: \n",
            "<START> as you world's is quite br mankind most that quest are chase to being quickly of little it time hell to plot br of something long put are of every place this consequence <UNK> of interplay storytelling being nasty not of you warren in is failed club i i of films pay so sequences <UNK> film okay uses to received <UNK> if time done for room sugar viewer as cartoon of gives to forgettable br be because many these of reflection sugar contained gives it wreck scene to more was two when had find as you another it of themselves probably who interplay storytelling if itself by br about 1950's films not would effects that her box to miike for if hero close seek end is very together movie of wheel got say kong sugar fred close bore there is playing lot of <UNK> pan place trilogy of lacks br of their time much this men as on it is telling program br silliness okay <UNK> to frustration at corner <UNK> she of sequences to political clearly in of drugs keep guy i i was throwing room sugar as it by br be plot many for occasionally film verge boyfriend difficult kid as you it failed not if gerard to if woman in <UNK> is police fi spooky or of self what have pretty in can so suit you good 2 which why super as it main of my i i  if time screenplay in same this remember assured have action one in realistic that better of lessons\n",
            " class:  {1}\n",
            "Predicted Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation function"
      ],
      "metadata": {
        "id": "NKE07hFr-5FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ],
      "metadata": {
        "id": "KaP0Iz-jzWTS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get baseline results\n",
        "baseline_results = calculate_results(y_true=y_test,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "CQWtz566zWXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47290244-9bbf-4008-98a4-02adbbc8ae0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 83.848,\n",
              " 'precision': 0.8399644774644901,\n",
              " 'recall': 0.83848,\n",
              " 'f1': 0.8383034853580418}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 : LSTM"
      ],
      "metadata": {
        "id": "pT7tdmTi_06Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Vectorizer"
      ],
      "metadata": {
        "id": "k-l8r9XRBiek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average number of tokens (words) in the training sets\n",
        "round(sum([len(i.split()) for i in x_train_text])/len(x_train_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIPDavNqCzrd",
        "outputId": "966e5cf9-6ea1-4bd3-c5b1-2531fda3d503"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorizer = TextVectorization(max_tokens = 10000,\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "\n",
        "                                    split = \"whitespace\",ngrams = None,\n",
        "                                    output_mode =\"int\",\n",
        "                                    output_sequence_length = 239)"
      ],
      "metadata": {
        "id": "L2C5ddbLBlcX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer.adapt(x_train_text)"
      ],
      "metadata": {
        "id": "7zZ4DievDCmz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,),dtype = \"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "print(x.shape)\n",
        "x = layers.LSTM(64,return_sequences=True)(x)\n",
        "print(x.shape)\n",
        "x = layers.LSTM(64)(x)\n",
        "print(x.shape)\n",
        "x = layers.Dense(64,activation = \"relu\")(x)\n",
        "print(x.shape)\n",
        "outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n",
        "print(outputs.shape)\n",
        "model_2 = tf.keras.Model(inputs,outputs,name = \"model_2_LSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU3wM-dtBGO6",
        "outputId": "0e69e09a-b0db-4245-9f90-16e47987ba12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 239, 32)\n",
            "(None, 239, 64)\n",
            "(None, 64)\n",
            "(None, 64)\n",
            "(None, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn99ckzCBGQy",
        "outputId": "f71f39c8-45c8-4674-b481-2307238de8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 239)               0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 239, 32)           320000    \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 239, 64)           24832     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 382081 (1.46 MB)\n",
            "Trainable params: 382081 (1.46 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(loss=\"binary_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "mrJtgtMkGnhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor = tf.convert_to_tensor(x_train_text,dtype = tf.string)\n"
      ],
      "metadata": {
        "id": "6QAXqGeTIpI1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.fit(x_train_tensor, y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuQK8awMGnjv",
        "outputId": "065664ea-c9a6-42a1-bac3-73bd868ff63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 53s 140ms/step - loss: 0.6826 - accuracy: 0.5528 - val_loss: 0.6716 - val_accuracy: 0.6072\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 21s 66ms/step - loss: 0.6858 - accuracy: 0.5294 - val_loss: 0.6933 - val_accuracy: 0.4938\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.6922 - accuracy: 0.5109 - val_loss: 0.6966 - val_accuracy: 0.4938\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.6812 - accuracy: 0.5394 - val_loss: 0.6872 - val_accuracy: 0.5500\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6731 - accuracy: 0.5537 - val_loss: 0.6936 - val_accuracy: 0.5062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f793f4132e0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 : Convo1D"
      ],
      "metadata": {
        "id": "0RIf8rISVVKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model_3_embedding = layers.Embedding(input_dim = 10000,\n",
        "                                   embeddings_initializer = \"uniform\",\n",
        "                                   output_dim = 128, input_length = 239)\n",
        "inputs = layers.Input(shape = (1,),dtype = \"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_3_embedding(x)\n",
        "x = layers.Conv1D(filters = 32,kernel_size = 5, activation = \"relu\")(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "model_3 = tf.keras.Model(inputs,outputs,name = \"model_3_Conv1D\")\n",
        "model_3.compile(loss=\"binary_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "6ZDB5nEMVY7B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk6ENMPTVZBz",
        "outputId": "744104a5-dead-432d-9cc4-eb835ec4e5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3_Conv1D\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVe  (None, 239)               0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 239, 128)          1280000   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 235, 32)           20512     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 32)                0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1300545 (4.96 MB)\n",
            "Trainable params: 1300545 (4.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_history = model_3.fit(x_train_tensor, y_train, epochs=5,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQr5mqs3Wouk",
        "outputId": "1507cf4f-15c9-4cd1-c09a-c1bc15a9149a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 37s 50ms/step - loss: 0.4617 - accuracy: 0.7818 - val_loss: 0.3389 - val_accuracy: 0.8604\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 11s 17ms/step - loss: 0.2462 - accuracy: 0.9003 - val_loss: 0.3076 - val_accuracy: 0.8734\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.1188 - accuracy: 0.9625 - val_loss: 0.3195 - val_accuracy: 0.8716\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.0428 - accuracy: 0.9924 - val_loss: 0.3488 - val_accuracy: 0.8736\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0138 - accuracy: 0.9991 - val_loss: 0.3884 - val_accuracy: 0.8802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_tensor = tf.convert_to_tensor(x_test_text,dtype = tf.string)\n"
      ],
      "metadata": {
        "id": "jiYd69QPXqZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "model_3_preds = model_3.predict(x_test_tensor)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxINl6IqXjhC",
        "outputId": "65db4ddb-f0b5-4866-9b98-204e062431a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr7mCRnXYe1l",
        "outputId": "8205fe1f-34a5-483e-ce3b-bcbe62e89000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.6922891e-02],\n",
              "       [9.9993908e-01],\n",
              "       [9.9032623e-01],\n",
              "       [6.4179882e-02],\n",
              "       [9.9987173e-01],\n",
              "       [6.0151058e-01],\n",
              "       [6.9247141e-02],\n",
              "       [1.5650665e-05],\n",
              "       [9.9982625e-01],\n",
              "       [7.3035580e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VnP0W6P3Yug5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds = np.where(model_3_preds > 0.5, 1, 0).flatten()\n",
        "\n",
        "# Get baseline results\n",
        "model_3_results = calculate_results(y_true=y_test, y_pred=model_3_preds)\n",
        "print(model_3_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d5qrHjY62r",
        "outputId": "661f345e-2ca9-4347-dd71-66ce42416aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 86.3, 'precision': 0.8632395428206834, 'recall': 0.863, 'f1': 0.8629774096884346}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def decode_label(label):\n",
        "    return 'Positive' if label == 1 else 'Negative'\n",
        "\n",
        "random_index = random.randint(0, len(x_test) - 1)\n",
        "sample_review = x_test[random_index]\n",
        "sample_review_text = decode_review(sample_review)\n",
        "\n",
        "\n",
        "actual_class = decode_label(y_test[random_index])\n",
        "\n",
        "print(f\"Sample Review Text: \\n{sample_review_text}\\n\", \"Class: \", actual_class)\n",
        "\n",
        "# Prepare the sample review as a batch of one\n",
        "sample_tensor = tf.convert_to_tensor([sample_review_text], dtype=tf.string)\n",
        "\n",
        "# Predict the sentiment of the sample review\n",
        "sample_prediction = model_3.predict(sample_tensor)\n",
        "predicted_class = 'Positive' if sample_prediction[0] > 0.5 else 'Negative'\n",
        "\n",
        "print(f\"Predicted Sentiment: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D148EEV7dtB1",
        "outputId": "1fc200da-74f9-46e9-c5e8-b384f04a2c57"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Review Text: \n",
            "<START> writing display on not so master material final own that material is among some br didn't was one of arrived to of 1970 another <UNK> it otherwise was least of on actors gore to me in season shame in start when that with has was halloween has often of material to one he's me in joel that that <UNK> or endure cinematic <UNK> in is hungry br <UNK> an 7 keep approached large to abuse who <UNK> like it of because michael <UNK> v ex conclusion <UNK> this of incredibly hot portrayal as fourth well 1 of <UNK> he offensive who <UNK> all big conflict time very movies <UNK> geek nonexistent like it is him driving to t\n",
            " Class:  Negative\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "sample_review_text = \" The movie was bad didn't like it \"\n",
        "\n",
        "print(f\"Sample Review Text: \\n{sample_review_text}\\n\")\n",
        "\n",
        "# Prepare the sample review as a batch of one\n",
        "sample_tensor = tf.convert_to_tensor([sample_review_text], dtype=tf.string)\n",
        "\n",
        "# Predict the sentiment of the sample review\n",
        "sample_prediction = model_3.predict(sample_tensor)\n",
        "predicted_class = 'Positive' if sample_prediction[0] > 0.5 else 'Negative'\n",
        "\n",
        "print(f\"Predicted Sentiment: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoTWCEOtdwiA",
        "outputId": "b953b097-8d99-4661-c471-dbe31b55f6cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Review Text: \n",
            " The movie was bad didn't like it \n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urdir4iwdwkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}